{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f0194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading new model: meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2d7ff8c807414a95aa2a6e09ca3589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-3B-Instruct into HookedTransformer\n",
      "New model loaded successfully on a single GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Sticking to a single CUDA device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the new model as per your selection\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load the model using HookedTransformer on a single device\n",
    "print(f\"Loading new model: {MODEL_NAME}...\")\n",
    "try:\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=device,\n",
    "        # Using float16 is crucial for fitting this on a single GPU\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    print(\"New model loaded successfully on a single GPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "    print(\"\\nIf this is an authentication error, you may need to log in to Hugging Face.\")\n",
    "    print(\"from huggingface_hub import login\")\n",
    "    print(\"login()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53baca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the CLEAN prompt and caching activations...\n",
      "Clean run cached successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7.1: Caching Activations from the Clean Run ---\n",
    "\n",
    "# We need the clean prompt and token IDs for the \"Abun\" puzzle.\n",
    "# Let's redefine them all here to have a self-contained block.\n",
    "\n",
    "# Prompts & Answers\n",
    "clean_question_abun = \"ji gwes\"\n",
    "clean_answer_abun = \" my leg\"\n",
    "corrupted_question_abun = \"ji bi gan\"\n",
    "corrupted_answer_abun = \" my offspring\"\n",
    "\n",
    "# Few-Shot Examples\n",
    "abun_few_shot_example_1 = \"\"\"---\n",
    "Abun: ji syim\n",
    "English: my arm\n",
    "---\"\"\"\n",
    "abun_few_shot_example_2 = \"\"\"---\n",
    "Abun: gap bi ngwe\n",
    "English: rat's garden\n",
    "---\"\"\"\n",
    "\n",
    "# Prompt Template\n",
    "abun_prompt_template_two_shot = \"\"\"Based on the following examples of Abun and English:\n",
    "\"Abun: gap sye bi gan\\nEnglish: big rat's offspring\"\n",
    "\"Abun: ji bi ngwe\\nEnglish: my garden\"\n",
    "\"Abun: ndar sye gwes\\nEnglish: big dog's leg\"\n",
    "\"Abun: gap bi ngwe\\nEnglish: rat's garden\"\n",
    "\n",
    "Complete the final translation, following the format of the examples.\n",
    "{example_1}\n",
    "{example_2}\n",
    "Abun: {question}\n",
    "English:\"\"\"\n",
    "\n",
    "# Construct the clean prompt\n",
    "clean_prompt_abun = abun_prompt_template_two_shot.format(\n",
    "    context=\"\", # Context is already in the template\n",
    "    example_1=abun_few_shot_example_1,\n",
    "    example_2=abun_few_shot_example_2,\n",
    "    question=clean_question_abun\n",
    ")\n",
    "\n",
    "# Construct the corrupted prompt (for the next step)\n",
    "corrupted_prompt_abun = abun_prompt_template_two_shot.format(\n",
    "    context=\"\",\n",
    "    example_1=abun_few_shot_example_1,\n",
    "    example_2=abun_few_shot_example_2,\n",
    "    question=corrupted_question_abun\n",
    ")\n",
    "\n",
    "\n",
    "# Get the token IDs using our robust method\n",
    "clean_answer_tokens = model.to_tokens(clean_answer_abun, prepend_bos=False)[0]\n",
    "clean_answer_token_id = clean_answer_tokens[-1].item()\n",
    "\n",
    "corrupted_answer_tokens = model.to_tokens(corrupted_answer_abun, prepend_bos=False)[0]\n",
    "corrupted_answer_token_id = corrupted_answer_tokens[-1].item()\n",
    "\n",
    "# --- Run with cache ---\n",
    "print(\"Running the model on the CLEAN prompt and caching activations...\")\n",
    "clean_logits, clean_cache = model.run_with_cache(\n",
    "    clean_prompt_abun,\n",
    "    remove_batch_dim=True\n",
    ")\n",
    "print(\"Clean run cached successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9899a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the CLEAN prompt and caching activations...\n",
      "Crucially, we are NOT removing the batch dimension this time.\n",
      "Clean run cached successfully. Example cached shape is now: torch.Size([1, 24, 127, 127])\n",
      "SUCCESS: Cached tensors now have the correct 4 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 11.1: Re-caching the Clean Run with Correct Dimensions ---\n",
    "\n",
    "print(\"Running the model on the CLEAN prompt and caching activations...\")\n",
    "print(\"Crucially, we are NOT removing the batch dimension this time.\")\n",
    "\n",
    "# --- Run with cache, REMOVING the `remove_batch_dim` argument ---\n",
    "clean_logits, clean_cache = model.run_with_cache(\n",
    "    clean_prompt_abun\n",
    ")\n",
    "\n",
    "# Verify the shape of a cached pattern tensor. It should now be 4D.\n",
    "example_pattern_shape = clean_cache[\"blocks.0.attn.hook_pattern\"].shape\n",
    "print(f\"Clean run cached successfully. Example cached shape is now: {example_pattern_shape}\")\n",
    "\n",
    "if len(example_pattern_shape) == 4:\n",
    "    print(\"SUCCESS: Cached tensors now have the correct 4 dimensions.\")\n",
    "else:\n",
    "    print(\"ERROR: Cached tensors are still not 4D. Please check the code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e90a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774c5bfc9664d1eafe2e57130b57567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Layers:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterative patching of attention patterns complete.\n",
      "Top 5 results sorted by logit difference:\n",
      "     layer  head  logit_diff\n",
      "200      8     8    3.925781\n",
      "64       2    16    3.328125\n",
      "126      5     6    3.011719\n",
      "321     13     9    2.996094\n",
      "537     22     9    2.816406\n"
     ]
    }
   ],
   "source": [
    "# --- Step 11.2: Final Patching Loop Execution ---\n",
    "import transformer_lens.utils as utils\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# This hook function correctly patches the 4D attention pattern.\n",
    "def pattern_patching_hook(\n",
    "    pattern,               # The attention pattern tensor [batch, n_heads, query_pos, key_pos]\n",
    "    hook,                  # The hook object\n",
    "    head_to_patch_index,   # The index of the head we are patching\n",
    "    clean_cache\n",
    "):\n",
    "    pattern[:, head_to_patch_index, :, :] = clean_cache[hook.name][:, head_to_patch_index, :, :]\n",
    "    return pattern\n",
    "\n",
    "# Get model configuration\n",
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "results = []\n",
    "\n",
    "# Loop over every head in the model\n",
    "for layer in tqdm(range(n_layers), desc=\"Layers\"):\n",
    "    for head in range(n_heads):\n",
    "        hook_fn = lambda pattern, hook: pattern_patching_hook(pattern, hook, head, clean_cache)\n",
    "        hook_point = utils.get_act_name(\"pattern\", layer)\n",
    "\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_prompt_abun,\n",
    "            fwd_hooks=[(hook_point, hook_fn)]\n",
    "        )\n",
    "\n",
    "        final_token_logits = patched_logits[0, -1, :]\n",
    "        logit_diff = (final_token_logits[clean_answer_token_id] - final_token_logits[corrupted_answer_token_id]).item()\n",
    "\n",
    "        results.append({\n",
    "            \"layer\": layer,\n",
    "            \"head\": head,\n",
    "            \"logit_diff\": logit_diff\n",
    "        })\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis.\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nIterative patching of attention patterns complete.\")\n",
    "print(\"Top 5 results sorted by logit difference:\")\n",
    "print(results_df.sort_values(\"logit_diff\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ef42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Candidate Heads for Ablation: [(8, 8), (2, 16), (5, 6), (13, 9), (22, 9)]\n",
      "\n",
      "Successfully created 5 hooks for targeted ablation.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 12: Defining the Targeted Ablation Mechanism ---\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "# The list of candidate reasoning heads we identified in Phase 1.\n",
    "# Format is (layer, head_index).\n",
    "candidate_heads_to_ablate = [\n",
    "    (8, 8),\n",
    "    (2, 16),\n",
    "    (5, 6),\n",
    "    (13, 9),\n",
    "    (22, 9)\n",
    "]\n",
    "print(f\"Identified Candidate Heads for Ablation: {candidate_heads_to_ablate}\")\n",
    "\n",
    "# This hook function zeroes out the attention pattern for a specific head.\n",
    "def zero_ablation_hook(\n",
    "    pattern,               # The attention pattern tensor\n",
    "    hook,                  # The hook object\n",
    "    head_to_zero_index     # The index of the head we want to zero out\n",
    "):\n",
    "    # Set the attention pattern for the target head to all zeros.\n",
    "    pattern[:, head_to_zero_index, :, :] = 0.0\n",
    "    return pattern\n",
    "\n",
    "# We need to create a unique hook function for each head we are ablating.\n",
    "# This factory function ensures that the correct 'head' index is captured by each hook.\n",
    "def create_hook_fn(head_idx_to_ablate):\n",
    "    # This returns a function that takes pattern and hook as input\n",
    "    return lambda pattern, hook: zero_ablation_hook(pattern, hook, head_idx_to_ablate)\n",
    "\n",
    "# Now, we build the list of hooks that will be permanently applied to the model.\n",
    "# We are ablating the 'pattern' hook point, the same one we patched.\n",
    "ablation_hooks = []\n",
    "for layer, head in candidate_heads_to_ablate:\n",
    "    hook_point = utils.get_act_name(\"pattern\", layer)\n",
    "    hook_function = create_hook_fn(head)\n",
    "    ablation_hooks.append((hook_point, hook_function))\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(ablation_hooks)} hooks for targeted ablation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de4416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the Reasoning Suite...\n",
      "Reasoning Suite created with 2 prompts.\n",
      "Example Reasoning Prompt:\n",
      "Based on the following examples of Mixe and English:\n",
      "\"Mixe: Ëjts nexp.\n",
      "English: I see.\n",
      "Mixe: Mejts mtunp.\n",
      "English: You work.\n",
      "Mixe: Juan yë'ë yexyejtpy. \n",
      "English: Juan watches him.\n",
      "Mixe: Yë'ë yë' uk yexpy.\n",
      "English: He sees the dog.\n",
      "Mixe: Ëjts yë' maxu'unk nexyejtpy.\n",
      "English: I watch the baby.\"\n",
      "\n",
      "Following the format of the examples, complete the final translation.\n",
      "Question: Mixe: Yë' maxu'unk yexp.\n",
      "Answer:\n",
      "\n",
      "Constructing the Recall Suite...\n",
      "Recall Suite created with 5 prompts.\n",
      "Example Recall Prompt:\n",
      "Based *only* on the text provided below:\n",
      "\"Mixe: Ëjts nexp.\n",
      "English: I see.\n",
      "Mixe: Mejts mtunp.\n",
      "English: You work.\n",
      "Mixe: Juan yë'ë yexyejtpy. \n",
      "English: Juan watches him.\n",
      "Mixe: Yë'ë yë' uk yexpy.\n",
      "English: He sees the dog.\n",
      "Mixe: Ëjts yë' maxu'unk nexyejtpy.\n",
      "English: I watch the baby.\"\n",
      "\n",
      "What is the English translation of the Mixe phrase \"Ëjts nexp.\"?\n"
     ]
    }
   ],
   "source": [
    "# --- Step 13: Preparing the Reasoning and Recall Test Suites ---\n",
    "\n",
    "# We will use the \"Ayutla Mixe\" puzzle data for our test suites.\n",
    "# This ensures we are not testing on the same data used to find the circuits.\n",
    "mixe_puzzle_data = {\n",
    "    \"context\": [\n",
    "        \"Mixe: Ëjts nexp.\\nEnglish: I see.\",\n",
    "        \"Mixe: Mejts mtunp.\\nEnglish: You work.\",\n",
    "        \"Mixe: Juan yë'ë yexyejtpy. \\nEnglish: Juan watches him.\",\n",
    "        \"Mixe: Yë'ë yë' uk yexpy.\\nEnglish: He sees the dog.\",\n",
    "        \"Mixe: Ëjts yë' maxu'unk nexyejtpy.\\nEnglish: I watch the baby.\"\n",
    "    ],\n",
    "    \"reasoning_questions\": [\n",
    "        {\"question\": \"Mixe: Yë' maxu'unk yexp.\", \"answer\": \"The baby sees.\"},\n",
    "        {\"question\": \"English: I work for him.\", \"answer\": \"Ëjts yë'ë nexpy.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- 1. Construct the Reasoning Suite ---\n",
    "print(\"Constructing the Reasoning Suite...\")\n",
    "reasoning_suite = []\n",
    "# We use a one-shot format to guide the model.\n",
    "reasoning_prompt_template = \"\"\"Based on the following examples of Mixe and English:\n",
    "\"{context_str}\"\n",
    "\n",
    "Following the format of the examples, complete the final translation.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "for task in mixe_puzzle_data[\"reasoning_questions\"]:\n",
    "    prompt = reasoning_prompt_template.format(\n",
    "        context_str=\"\\n\".join(mixe_puzzle_data[\"context\"]),\n",
    "        question=task[\"question\"]\n",
    "    )\n",
    "    reasoning_suite.append({\"prompt\": prompt, \"correct_answer\": task[\"answer\"]})\n",
    "\n",
    "print(f\"Reasoning Suite created with {len(reasoning_suite)} prompts.\")\n",
    "print(\"Example Reasoning Prompt:\")\n",
    "print(reasoning_suite[0]['prompt'])\n",
    "\n",
    "\n",
    "# --- 2. Construct the Recall Suite ---\n",
    "print(\"\\nConstructing the Recall Suite...\")\n",
    "recall_suite = []\n",
    "# The recall prompt explicitly asks to retrieve information from the text.\n",
    "recall_prompt_template = \"\"\"Based *only* on the text provided below:\n",
    "\"{context_str}\"\n",
    "\n",
    "What is the English translation of the Mixe phrase \"{mixe_phrase}\"?\"\"\"\n",
    "\n",
    "# We generate recall tasks from the examples themselves.\n",
    "for example in mixe_puzzle_data[\"context\"]:\n",
    "    if \"Mixe:\" in example and \"English:\" in example:\n",
    "        parts = example.split(\"\\n\")\n",
    "        mixe_part = parts[0].replace(\"Mixe: \", \"\").strip()\n",
    "        english_part = parts[1].replace(\"English: \", \"\").strip()\n",
    "\n",
    "        prompt = recall_prompt_template.format(\n",
    "            context_str=\"\\n\".join(mixe_puzzle_data[\"context\"]),\n",
    "            mixe_phrase=mixe_part\n",
    "        )\n",
    "        recall_suite.append({\"prompt\": prompt, \"correct_answer\": english_part})\n",
    "\n",
    "print(f\"Recall Suite created with {len(recall_suite)} prompts.\")\n",
    "print(\"Example Recall Prompt:\")\n",
    "print(recall_suite[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db9fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Original Model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ec8986351240c4801fe029e16a80ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (Ablated=False):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e9bdb123864f799356a06976a3136c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc68457a56894089914435bf6ae1aa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15cb01915044b86a9230f7b91725122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (Ablated=False):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dd3ba9895547ad9cefd9a6214069c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffa123791a84119a7047e2552093663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6ac3656cb844fea5b2a2f66208daf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9bc3c1820e46098b9237f53d348c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bc51e467fe4495be0a6f2ad929d94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Ablated Model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a262512bbf7f449eb5f6f415459c9229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (Ablated=True):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33882ad4ab2e42f6bfc05c5bec7ac4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cfdbec3cb2492dbb850b27663beb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dcbc1f10c44834a3a594a0d468957a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (Ablated=True):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5a53af638a4c0d985d64896ff32ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b34a1e217a345778df7f361ba8dde42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0bca3219ef4fcb9a3b963a86d4955d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaee0bea50ec4171af5726ebd5473727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987590e3f83547078a162a4cdba78dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- FINAL RESULTS ---\n",
      "    Suite  Original Model Accuracy (%)  Ablated Model Accuracy (%)  Performance Drop (%)\n",
      "Reasoning                          0.0                         0.0                   0.0\n",
      "   Recall                        100.0                       100.0                   0.0\n",
      "\n",
      "--- Validation ---\n",
      "FAILURE: The results do not align with the hypothesis.\n",
      "The ablated heads may not be as specialized for reasoning as predicted.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 14 (Final Corrected Version): Performance Evaluation and Analysis ---\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function to check if the model's answer is correct.\n",
    "def is_correct(generated_text, correct_answer):\n",
    "    # We check if the correct answer appears in the generated text, ignoring case.\n",
    "    # A more robust solution might involve normalizing text, but this is good for our purpose.\n",
    "    return correct_answer.lower() in generated_text.lower()\n",
    "\n",
    "# The corrected evaluation function using the model.hooks() context manager.\n",
    "def evaluate_performance(suite, use_ablation_hooks=False):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(suite)\n",
    "    \n",
    "    hooks_to_use = ablation_hooks if use_ablation_hooks else []\n",
    "\n",
    "    for task in tqdm(suite, desc=f\"Evaluating (Ablated={use_ablation_hooks})\"):\n",
    "        prompt = task[\"prompt\"]\n",
    "        correct_answer = task[\"correct_answer\"]\n",
    "        \n",
    "        # Use the model.hooks context manager to apply hooks to the .generate() call\n",
    "        with model.hooks(fwd_hooks=hooks_to_use):\n",
    "            output = model.generate(\n",
    "                prompt,\n",
    "                max_new_tokens=15,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        if is_correct(output, correct_answer):\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    return (correct_predictions / total_predictions) * 100\n",
    "\n",
    "# --- Run the four evaluations ---\n",
    "print(\"--- Evaluating Original Model ---\")\n",
    "original_reasoning_accuracy = evaluate_performance(reasoning_suite, use_ablation_hooks=False)\n",
    "original_recall_accuracy = evaluate_performance(recall_suite, use_ablation_hooks=False)\n",
    "\n",
    "print(\"\\n--- Evaluating Ablated Model ---\")\n",
    "ablated_reasoning_accuracy = evaluate_performance(reasoning_suite, use_ablation_hooks=True)\n",
    "ablated_recall_accuracy = evaluate_performance(recall_suite, use_ablation_hooks=True)\n",
    "\n",
    "# --- Analyze and display the results ---\n",
    "performance_drop_reasoning = original_reasoning_accuracy - ablated_reasoning_accuracy\n",
    "performance_drop_recall = original_recall_accuracy - ablated_recall_accuracy\n",
    "\n",
    "results = {\n",
    "    \"Suite\": [\"Reasoning\", \"Recall\"],\n",
    "    \"Original Model Accuracy (%)\": [original_reasoning_accuracy, original_recall_accuracy],\n",
    "    \"Ablated Model Accuracy (%)\": [ablated_reasoning_accuracy, ablated_recall_accuracy],\n",
    "    \"Performance Drop (%)\": [performance_drop_reasoning, performance_drop_recall]\n",
    "}\n",
    "summary_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n--- FINAL RESULTS ---\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# --- Final Validation based on Success Criterion ---\n",
    "print(\"\\n--- Validation ---\")\n",
    "if performance_drop_reasoning > 20 and performance_drop_recall < 10:\n",
    "    print(\"SUCCESS: The results align with the hypothesis.\")\n",
    "    print(\"Ablating the candidate heads significantly harms reasoning performance while leaving recall largely intact.\")\n",
    "    print(\"This provides strong evidence that these heads form a specialized reasoning circuit.\")\n",
    "else:\n",
    "    print(\"FAILURE: The results do not align with the hypothesis.\")\n",
    "    print(\"The ablated heads may not be as specialized for reasoning as predicted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aa5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
