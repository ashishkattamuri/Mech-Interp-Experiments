{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba79c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization successful. Shapes are now consistent:\n",
      "Clean inputs shape: torch.Size([1, 96])\n",
      "Corrupted inputs shape: torch.Size([1, 96])\n",
      "\n",
      "--- Getting activation from Layer 25 using clean prompt ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Above exception when execution Node: 'ApplyModuleProtocol_0' in Graph: '134591725275552'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/tracing/Node.py:372\u001b[0m, in \u001b[0;36mNode.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, protocols\u001b[38;5;241m.\u001b[39mProtocol\n\u001b[1;32m    370\u001b[0m ):\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m \n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# Prepare arguments.\u001b[39;00m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/tracing/protocols.py:152\u001b[0m, in \u001b[0;36mApplyModuleProtocol.execute\u001b[0;34m(cls, node)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m node\u001b[38;5;241m.\u001b[39mset_value(output)\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:446\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:531\u001b[0m, in \u001b[0;36mQwen3Model._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (using_static_cache \u001b[38;5;129;01mor\u001b[39;00m using_sliding_window_cache)\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions\n\u001b[1;32m    530\u001b[0m ):\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ignore_causal_mask_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_seen_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:288\u001b[0m, in \u001b[0;36mAttentionMaskConverter._ignore_causal_mask_sdpa\u001b[0;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(attention_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key_value_length \u001b[38;5;241m==\u001b[39m query_length:\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;66;03m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[39;00m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/torch/_meta_registrations.py:7088\u001b[0m, in \u001b[0;36mmeta_local_scalar_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7086\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(aten\u001b[38;5;241m.\u001b[39m_local_scalar_dense)\n\u001b[1;32m   7087\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmeta_local_scalar_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Tensor):\n\u001b[0;32m-> 7088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.item() cannot be called on meta tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor.item() cannot be called on meta tensors",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Getting activation from Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using clean prompt ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# --- STEP 1: Use llm.trace() and a direct model call to get the activation ---\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# This is the most fundamental way to trace execution.\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mtrace(validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Directly call the underlying Hugging Face model's forward pass.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     llm\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclean_inputs)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Define the save operation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/contexts/Tracer.py:102\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/contexts/GraphBasedContext.py:217\u001b[0m, in \u001b[0;36mGraphBasedContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/contexts/backends/LocalBackend.py:27\u001b[0m, in \u001b[0;36mLocalBackend.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: LocalMixin):\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_backend_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/contexts/Tracer.py:144\u001b[0m, in \u001b[0;36mTracer.local_backend_execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocols\u001b[38;5;241m.\u001b[39mBridgeProtocol\u001b[38;5;241m.\u001b[39mhas_bridge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph):\n\u001b[1;32m    142\u001b[0m     invoker_inputs \u001b[38;5;241m=\u001b[39m resolve_dependencies(invoker_inputs)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minterleave(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_execute,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39minvoker_inputs,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs,\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/tracing/Graph.py:104\u001b[0m, in \u001b[0;36mGraph.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m root_nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    100\u001b[0m     node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mfulfilled()\n\u001b[1;32m    101\u001b[0m ]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m root_nodes:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/nnsight/tracing/Node.py:387\u001b[0m, in \u001b[0;36mNode.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_value(output)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbove exception when execution Node: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in Graph: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremaining_dependencies \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Above exception when execution Node: 'ApplyModuleProtocol_0' in Graph: '134591725275552'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "import json\n",
    "\n",
    "# --- Load the MODELING dataset ---\n",
    "# Make sure you have a file named 'modeling_dataset.json' in the same directory\n",
    "# or provide the correct full path to the file.\n",
    "try:\n",
    "    with open('modeling_dataset.json', 'r') as f:\n",
    "        modeling_dataset = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'modeling_dataset.json' not found. Please ensure the dataset file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Load the model ---\n",
    "llm = LanguageModel(\"Qwen/Qwen3-1.7B\", device_map=\"auto\")\n",
    "if llm.tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer does not have a pad token, setting it to eos_token.\")\n",
    "    llm.tokenizer.pad_token = llm.tokenizer.eos_token\n",
    "\n",
    "\n",
    "# --- Select a problem to analyze ---\n",
    "problem = modeling_dataset['problems'][0]\n",
    "\n",
    "# --- Prepare the prompts ---\n",
    "question_index = 0\n",
    "question = problem['questions'][question_index]\n",
    "correct_answer = problem['answers'][question_index]\n",
    "\n",
    "examples = \"\\n\".join(problem['data'])\n",
    "clean_prompt_text = f\"Translate based on the examples:\\n{examples}\\n\\n{question}\"\n",
    "\n",
    "corrupted_example = \"Abun: gwes ji\\nEnglish: my leg\"\n",
    "corrupted_examples = examples + \"\\n\" + corrupted_example\n",
    "corrupted_prompt_text = f\"Translate based on the examples:\\n{corrupted_examples}\\n\\n{question}\"\n",
    "\n",
    "# --- Tokenize together with padding to ensure identical shapes ---\n",
    "prompts = [clean_prompt_text, corrupted_prompt_text]\n",
    "inputs = llm.tokenizer(prompts, return_tensors='pt', padding=True).to(llm.device)\n",
    "\n",
    "# Separate the tokenized inputs\n",
    "clean_inputs = {'input_ids': inputs['input_ids'][0:1], 'attention_mask': inputs['attention_mask'][0:1]}\n",
    "corrupted_inputs = {'input_ids': inputs['input_ids'][1:2], 'attention_mask': inputs['attention_mask'][1:2]}\n",
    "\n",
    "print(\"Tokenization successful. Shapes are now consistent:\")\n",
    "print(f\"Clean inputs shape: {clean_inputs['input_ids'].shape}\")\n",
    "print(f\"Corrupted inputs shape: {corrupted_inputs['input_ids'].shape}\")\n",
    "\n",
    "\n",
    "target_layer = 25\n",
    "print(f\"\\n--- Getting activation from Layer {target_layer} using clean prompt ---\")\n",
    "\n",
    "# --- STEP 1: Use llm.trace() and a direct model call to get the activation ---\n",
    "# This is the most fundamental way to trace execution.\n",
    "with llm.trace(validate=False) as tracer:\n",
    "    # Directly call the underlying Hugging Face model's forward pass.\n",
    "    llm.model(**clean_inputs)\n",
    "    # Define the save operation.\n",
    "    clean_activation_proxy = llm.model.layers[target_layer].self_attn.o_proj.output.save()\n",
    "\n",
    "# Get the concrete tensor value after the run is complete.\n",
    "clean_activation_value = clean_activation_proxy.value\n",
    "print(f\"Activation saved. Shape: {clean_activation_value.shape}\")\n",
    "print(f\"\\n--- Patching Layer {target_layer} and running corrupted prompt ---\")\n",
    "\n",
    "\n",
    "# --- STEP 2: Use a new trace context for the patching run ---\n",
    "with llm.trace(validate=False) as tracer:\n",
    "    # Get a proxy for the output we want to replace.\n",
    "    llm.model.layers[target_layer].self_attn.o_proj.output = clean_activation_value\n",
    "\n",
    "    # Directly call the model's generate method inside the trace.\n",
    "    output = llm.model.generate(\n",
    "        **corrupted_inputs,\n",
    "        max_new_tokens=15,\n",
    "        pad_token_id=llm.tokenizer.eos_token_id\n",
    "    ).save()\n",
    "\n",
    "\n",
    "# --- Analyze the output ---\n",
    "generated_tokens = output.value[0]\n",
    "generated_text = llm.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nOriginal question: {question}\")\n",
    "print(f\"Expected answer: {correct_answer}\")\n",
    "# We need to parse the answer from the full generated text\n",
    "full_text_of_corrupted_prompt = llm.tokenizer.decode(corrupted_inputs['input_ids'][0], skip_special_tokens=True)\n",
    "generated_answer = generated_text.split(full_text_of_corrupted_prompt)[-1].strip()\n",
    "print(f\"Generated output after patching: {generated_answer}\")\n",
    "\n",
    "if correct_answer.lower() in generated_answer.lower():\n",
    "    print(f\"\\nSUCCESS: Patching layer {target_layer} appears to have restored the correct reasoning!\")\n",
    "else:\n",
    "    print(f\"\\nINFO: Patching layer {target_layer} did not restore the correct answer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d570b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshwardhan/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with TransformerLens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-3B-Instruct into HookedTransformer\n",
      "Model loaded successfully.\n",
      "\n",
      "Targeting hook: 'blocks.25.attn.hook_z'\n",
      "Activation saved. Shape: torch.Size([1, 84, 16, 128])\n",
      "\n",
      "--- Patching Layer 25 and running corrupted prompt ---\n",
      "  Patching hook running at 'blocks.25.attn.hook_z'...\n",
      "\n",
      "Original question: Abun: ji gwes\n",
      "Expected answer: English: my leg\n",
      "Generated output after patching: \n",
      "\n",
      "INFO: Patching layer 25 did not restore the correct answer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import json\n",
    "\n",
    "# --- Load the MODELING dataset ---\n",
    "# Make sure you have a file named 'modeling_dataset.json' in the same directory\n",
    "# or provide the correct full path to the file.\n",
    "try:\n",
    "    with open('modeling_dataset.json', 'r') as f:\n",
    "        modeling_dataset = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'modeling_dataset.json' not found. Please ensure the dataset file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Load the model using TransformerLens ---\n",
    "# This might take some time on the first run as it downloads the model.\n",
    "# trust_remote_code=True is required for many community models.\n",
    "print(\"Loading model with TransformerLens...\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    fold_ln=False, # Important for interventions\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False\n",
    ")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# --- Select a problem to analyze ---\n",
    "problem = modeling_dataset['problems'][0]\n",
    "\n",
    "# --- Prepare the prompts ---\n",
    "question_index = 0\n",
    "question = problem['questions'][question_index]\n",
    "correct_answer = problem['answers'][question_index]\n",
    "\n",
    "examples = \"\\n\".join(problem['data'])\n",
    "clean_prompt_text = f\"Translate based on the examples:\\n{examples}\\n\\n{question}\"\n",
    "\n",
    "corrupted_example = \"Abun: gwes ji\\nEnglish: my leg\"\n",
    "corrupted_examples = examples + \"\\n\" + corrupted_example\n",
    "corrupted_prompt_text = f\"Translate based on the examples:\\n{corrupted_examples}\\n\\n{question}\"\n",
    "\n",
    "# --- Tokenize inputs ---\n",
    "# TransformerLens has its own tokenizer interface\n",
    "clean_tokens = model.to_tokens(clean_prompt_text)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt_text)\n",
    "\n",
    "target_layer = 25\n",
    "# The hook name for the output of the attention block in TransformerLens\n",
    "# is 'blocks.{layer_index}.attn.hook_z'\n",
    "hook_name = f\"blocks.{target_layer}.attn.hook_z\"\n",
    "print(f\"\\nTargeting hook: '{hook_name}'\")\n",
    "\n",
    "\n",
    "# --- STEP 1: Get the clean activation ---\n",
    "# Run the model on the clean prompt and cache all activations\n",
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "\n",
    "# Retrieve the specific activation we want to save\n",
    "clean_activation_value = clean_cache[hook_name]\n",
    "print(f\"Activation saved. Shape: {clean_activation_value.shape}\")\n",
    "\n",
    "\n",
    "# --- STEP 2: Define the patching hook and run with it ---\n",
    "\n",
    "# This hook function will be called during the forward pass\n",
    "def patching_hook(activation_at_hook, hook):\n",
    "    print(f\"  Patching hook running at '{hook.name}'...\")\n",
    "    \n",
    "    # Get the sequence length of the shorter, clean activation\n",
    "    clean_len = clean_activation_value.shape[1]\n",
    "    \n",
    "    # We replace the *beginning* of the corrupted activation\n",
    "    # with the *entire* clean activation.\n",
    "    activation_at_hook[:, :clean_len, :] = clean_activation_value\n",
    "    \n",
    "    return activation_at_hook\n",
    "\n",
    "print(f\"\\n--- Patching Layer {target_layer} and running corrupted prompt ---\")\n",
    "\n",
    "# Run the model on the corrupted prompt, but with our patching hook active.\n",
    "# The hook will modify the forward pass in flight.\n",
    "patched_logits = model.run_with_hooks(\n",
    "    corrupted_tokens,\n",
    "    fwd_hooks=[(hook_name, patching_hook)]\n",
    ")\n",
    "\n",
    "# --- Analyze the output ---\n",
    "# Get the token with the highest logit at the last position\n",
    "last_token_logits = patched_logits[0, -1, :]\n",
    "predicted_token_id = torch.argmax(last_token_logits).item()\n",
    "\n",
    "# THE FIX: Use model.cfg.device to get the correct device.\n",
    "predicted_token_tensor = torch.tensor([predicted_token_id]).to(model.cfg.device)\n",
    "\n",
    "# Decode the entire corrupted input + the predicted token\n",
    "full_generated_ids = torch.cat([corrupted_tokens[0], predicted_token_tensor])\n",
    "generated_text = model.to_string(full_generated_ids)\n",
    "\n",
    "print(f\"\\nOriginal question: {question}\")\n",
    "print(f\"Expected answer: {correct_answer}\")\n",
    "# We parse the generated answer from the full text\n",
    "generated_answer = generated_text.split(corrupted_prompt_text)[-1].strip()\n",
    "print(f\"Generated output after patching: {generated_answer}\")\n",
    "\n",
    "if correct_answer.lower() in generated_answer.lower():\n",
    "    print(f\"\\nSUCCESS: Patching layer {target_layer} appears to have restored the correct reasoning!\")\n",
    "else:\n",
    "    print(f\"\\nINFO: Patching layer {target_layer} did not restore the correct answer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97107919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with TransformerLens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-3B-Instruct into HookedTransformer\n",
      "Model loaded successfully.\n",
      "Analyzing logits for the first token of the answer: 'English' (ID: 22574)\n",
      "\n",
      "--- Running Baseline Prompts (No Patching) ---\n",
      "Clean Prompt Logit for 'English': 16.3440\n",
      "Corrupted Prompt Logit for 'English': 14.9726\n",
      "\n",
      "--- Running Patching Experiment ---\n",
      "Layer 00 | Patched Logit: 17.0854 | Recovery: 154.06%\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 11.62 MiB is free. Process 2399 has 1.59 GiB memory in use. Process 7013 has 1.25 GiB memory in use. Process 935982 has 442.00 MiB memory in use. Process 1042199 has 572.00 MiB memory in use. Process 1874016 has 368.00 MiB memory in use. Process 2657807 has 3.14 GiB memory in use. Including non-PyTorch memory, this process has 40.02 GiB memory in use. Of the allocated memory 38.66 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m hook_name_template\u001b[38;5;241m.\u001b[39mformat(layer\u001b[38;5;241m=\u001b[39mlayer)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Run the model on the corrupted prompt with the patching hook active.\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m patched_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrupted_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatching_hook\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m patched_logit_val \u001b[38;5;241m=\u001b[39m patched_logits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, correct_answer_first_token_id]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calculate how much of the \"damage\" was recovered\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# 100% recovery means the patched logit is the same as the clean logit\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# 0% recovery means the patched logit is the same as the corrupted logit\u001b[39;00m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformer_lens/hook_points.py:456\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:612\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    609\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    610\u001b[0m         )\n\u001b[0;32m--> 612\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:186\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m     mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    183\u001b[0m         resid_mid \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n\u001b[0;32m--> 186\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid_mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_post(resid_mid \u001b[38;5;241m+\u001b[39m mlp_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:210\u001b[0m, in \u001b[0;36mTransformerBlock.apply_mlp\u001b[0;34m(self, normalized_resid)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_mlp\u001b[39m(\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m, normalized_resid: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Centralized point where the MLP is applied to the forward pass\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        Float[torch.Tensor, \"batch pos d_model\"]: Our resulting tensor\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    212\u001b[0m         mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2_post(mlp_out)\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/transformer_lens/components/mlps/gated_mlp.py:72\u001b[0m, in \u001b[0;36mGatedMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     pre_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_pre_linear(\n\u001b[1;32m     68\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_in)  \u001b[38;5;66;03m# batch pos d_model, d_model d_mlp -> batch pos d_mlp\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     post_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_post(\n\u001b[0;32m---> 72\u001b[0m         (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_act\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_linear\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_in\n\u001b[1;32m     73\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_addmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_out, post_act)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 11.62 MiB is free. Process 2399 has 1.59 GiB memory in use. Process 7013 has 1.25 GiB memory in use. Process 935982 has 442.00 MiB memory in use. Process 1042199 has 572.00 MiB memory in use. Process 1874016 has 368.00 MiB memory in use. Process 2657807 has 3.14 GiB memory in use. Including non-PyTorch memory, this process has 40.02 GiB memory in use. Of the allocated memory 38.66 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Load the MODELING dataset ---\n",
    "# Make sure you have a file named 'modeling_dataset.json' in the same directory\n",
    "# or provide the correct full path to the file.\n",
    "try:\n",
    "    with open('modeling_dataset.json', 'r') as f:\n",
    "        modeling_dataset = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'modeling_dataset.json' not found. Please ensure the dataset file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Load the model using TransformerLens ---\n",
    "print(\"Loading model with TransformerLens...\")\n",
    "# UPDATED: Using the correct model name as requested\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    fold_ln=False, \n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False\n",
    ")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# --- Select a problem to analyze ---\n",
    "problem = modeling_dataset['problems'][1]\n",
    "\n",
    "# --- Prepare the prompts ---\n",
    "question_index = 0\n",
    "question = problem['questions'][question_index]\n",
    "correct_answer = problem['answers'][question_index]\n",
    "\n",
    "examples = \"\\n\".join(problem['data'])\n",
    "clean_prompt_text = f\"Translate based on the examples:\\n{examples}\\n\\n{question}\"\n",
    "\n",
    "corrupted_example = \"Abun: gwes ji\\nEnglish: my leg\"\n",
    "corrupted_examples = examples + \"\\n\" + corrupted_example\n",
    "corrupted_prompt_text = f\"Translate based on the examples:\\n{corrupted_examples}\\n\\n{question}\"\n",
    "\n",
    "# --- Tokenize all inputs ---\n",
    "clean_tokens = model.to_tokens(clean_prompt_text)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt_text)\n",
    "\n",
    "# --- THE FIX: Identify the VERY FIRST token of the correct answer for logit analysis ---\n",
    "# Tokenize the whole correct answer\n",
    "correct_answer_tokens = model.to_tokens(correct_answer, prepend_bos=False)\n",
    "# Select only the first token ID\n",
    "correct_answer_first_token_id = correct_answer_tokens[0, 0]\n",
    "# Decode just that single token for a clean printout\n",
    "correct_answer_first_token_str = model.to_string([correct_answer_first_token_id])\n",
    "\n",
    "print(f\"Analyzing logits for the first token of the answer: '{correct_answer_first_token_str}' (ID: {correct_answer_first_token_id.item()})\")\n",
    "\n",
    "\n",
    "# --- RUN THE EXPERIMENT ---\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- 1. Get Baseline Logits (Clean and Corrupted without patching) ---\n",
    "print(\"\\n--- Running Baseline Prompts (No Patching) ---\")\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits = model(corrupted_tokens)\n",
    "\n",
    "# Get the logit of the correct answer token from the last position\n",
    "clean_logit_val = clean_logits[0, -1, correct_answer_first_token_id].item()\n",
    "corrupted_logit_val = corrupted_logits[0, -1, correct_answer_first_token_id].item()\n",
    "\n",
    "print(f\"Clean Prompt Logit for '{correct_answer_first_token_str}': {clean_logit_val:.4f}\")\n",
    "print(f\"Corrupted Prompt Logit for '{correct_answer_first_token_str}': {corrupted_logit_val:.4f}\")\n",
    "\n",
    "\n",
    "# --- 2. Run the Patching Experiment Across All Layers ---\n",
    "print(\"\\n--- Running Patching Experiment ---\")\n",
    "\n",
    "# The hook name for the output of the attention block in TransformerLens\n",
    "# is 'blocks.{layer_index}.attn.hook_z'\n",
    "hook_name_template = \"blocks.{layer}.attn.hook_z\"\n",
    "\n",
    "# This hook function will be called during the forward pass\n",
    "def patching_hook(activation_at_hook, hook):\n",
    "    # Get the sequence length of the shorter, clean activation\n",
    "    clean_len = clean_cache[hook.name].shape[1]\n",
    "    \n",
    "    # Replace the *beginning* of the corrupted activation with the *entire* clean activation.\n",
    "    activation_at_hook[:, :clean_len, :] = clean_cache[hook.name]\n",
    "    \n",
    "    return activation_at_hook\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    hook_name = hook_name_template.format(layer=layer)\n",
    "    \n",
    "    # Run the model on the corrupted prompt with the patching hook active.\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        corrupted_tokens,\n",
    "        fwd_hooks=[(hook_name, patching_hook)]\n",
    "    )\n",
    "    \n",
    "    patched_logit_val = patched_logits[0, -1, correct_answer_first_token_id].item()\n",
    "    \n",
    "    # Calculate how much of the \"damage\" was recovered\n",
    "    # 100% recovery means the patched logit is the same as the clean logit\n",
    "    # 0% recovery means the patched logit is the same as the corrupted logit\n",
    "    recovery_percentage = ((patched_logit_val - corrupted_logit_val) / (clean_logit_val - corrupted_logit_val)) * 100\n",
    "    \n",
    "    print(f\"Layer {layer:02d} | Patched Logit: {patched_logit_val:.4f} | Recovery: {recovery_percentage:.2f}%\")\n",
    "    results.append({'layer': layer, 'recovery': recovery_percentage})\n",
    "\n",
    "# --- 3. Visualize the Results ---\n",
    "print(\"\\n--- Experiment Complete. Visualizing Results... ---\")\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Ensure the 'layer' column is treated as a categorical variable for correct sorting\n",
    "df['layer'] = df['layer'].astype(str).str.zfill(2)\n",
    "\n",
    "fig = px.bar(\n",
    "    df, \n",
    "    x='layer', \n",
    "    y='recovery', \n",
    "    title='Logit Recovery Percentage by Patched Layer',\n",
    "    labels={'layer': 'Transformer Layer', 'recovery': 'Logit Recovery (%)'},\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Logit Recovery (%)\",\n",
    "    xaxis_title=\"Layer Index (Attention Output 'hook_z')\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb595a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
