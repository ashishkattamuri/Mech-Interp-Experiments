{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f87a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshwardhan/Interpretable_Transformers/harshenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233a3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LanguageModel(\"Qwen/Qwen3-0.6B\", device_map=\"auto\", dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d7253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe062ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"When John and Mary went to the shops, John gave the bag to\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
    "    \"When Tom and James went to the park, James gave the ball to\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
    "]\n",
    "\n",
    "# Answers are each formatted as (correct, incorrect):\n",
    "answers = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" John\", \" Mary\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" James\", \" Tom\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Sid\", \" Dan\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "    (\" Amy\", \" Martin\"),\n",
    "]\n",
    "\n",
    "clean_tokens=llm.tokenizer(prompts,return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
    "]\n",
    "\n",
    "answer_token_indices = torch.tensor(\n",
    "    [\n",
    "        [llm.tokenizer(answers[i][j])[\"input_ids\"][0] for j in range(2)]\n",
    "        for i in range(len(answers))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c41507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 6.0948\n",
      "Corrupted logit diff: -6.0948\n"
     ]
    }
   ],
   "source": [
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "\n",
    "clean_logits=llm.trace(clean_tokens,trace=False).logits.cpu()\n",
    "corrupted_logits=llm.trace(corrupted_tokens,trace=False).logits.cpu()\n",
    "CLEAN_BASELINE = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {CLEAN_BASELINE:.4f}\")\n",
    "\n",
    "CORRUPTED_BASELINE = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {CORRUPTED_BASELINE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb7c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Baseline is 1: 1.0000\n",
      "Corrupted Baseline is 0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def ioi_metric(\n",
    "    logits,\n",
    "    answer_token_indices=answer_token_indices,\n",
    "):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbffc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
